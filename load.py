#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import logging
import datetime as dt
import urllib.parse
from typing import Any, Dict, List, Optional, Tuple, Set


import pandas as pd
import requests
import urllib3
from dotenv import load_dotenv

import pyarrow as pa
from pyiceberg.catalog import load_catalog
from pyiceberg.table import Table
from pyiceberg.types import (
    BooleanType, IntegerType, LongType, DoubleType, FloatType, DateType, TimestampType,
    StringType, BinaryType, DecimalType, MapType, ListType, StructType, NestedField
)
from pyiceberg.expressions import And, EqualTo
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.INFO, format="%(asctime)s  %(levelname)-8s  %(message)s")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REQ = [
    "POSTGRES_USER", "POSTGRES_PASSWORD", "POSTGRES_DB",
    "AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY",
    "TRINO_SCHEMA", "TRINO_TABLE",
    "RIOT_API_KEY",
]
missing = [v for v in REQ if not os.getenv(v)]
if missing:
    raise EnvironmentError(f"Missing env vars: {', '.join(missing)}")

PG_HOST = os.getenv("POSTGRES_HOST", "postgres")
PG_PORT = int(os.getenv("POSTGRES_PORT", "5432"))
PG_DB   = os.environ["POSTGRES_DB"]
PG_USER = os.environ["POSTGRES_USER"]
PG_PASS = os.environ["POSTGRES_PASSWORD"]

AWS_ACCESS_KEY_ID     = os.environ["AWS_ACCESS_KEY_ID"]
AWS_SECRET_ACCESS_KEY = os.environ["AWS_SECRET_ACCESS_KEY"]
S3_ENDPOINT = os.getenv("S3_ENDPOINT", "https://storage.yandexcloud.net")
S3_REGION   = os.getenv("S3_REGION", "ru-central1")

SCHEMA_NAME = os.environ["TRINO_SCHEMA"]
TABLE_NAME  = os.environ["TRINO_TABLE"]
TABLE_ID    = f"{SCHEMA_NAME}.{TABLE_NAME}"

RIOT_API_KEY = os.environ["RIOT_API_KEY"]
PLATFORM_ROUTING = os.getenv("PLATFORM_ROUTING", "ru1")
REGIONAL_ROUTING = os.getenv("REGIONAL_ROUTING", "europe")

# Ğ’ĞµÑ€Ñ…Ğ½ĞµÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ¼Ğ°Ñ‚Ñ‡Ğ°
META_COLS = [
    "metadata.matchId",
    "info.gameCreation",
    "info.gameDuration",
    "info.gameMode",
    "info.queueId",
    "info.gameVersion",
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³/Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def open_catalog() -> Table:
    uri = f"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}"
    # Ğ”ĞĞ›Ğ–ĞĞ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ Ñ iceberg.jdbc-catalog.catalog-name Ñƒ Trino
    catalog_name = os.getenv("ICEBERG_CATALOG_NAME", os.getenv("TRINO_CATALOG", "iceberg"))
    return load_catalog(
        catalog_name,
        **{
            "type": "sql",
            "uri": uri,
            "s3.endpoint": S3_ENDPOINT,
            "s3.region": S3_REGION,
            "s3.access-key-id": AWS_ACCESS_KEY_ID,
            "s3.secret-access-key": AWS_SECRET_ACCESS_KEY,
        },
    )

def load_table() -> Table:
    cat = open_catalog()
    tbl = cat.load_table(TABLE_ID)
    logging.info("âœ… Loaded Iceberg table: %s", TABLE_ID)
    return tbl

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Iceberg â†’ Arrow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def iceberg_to_arrow_type(t) -> pa.DataType:
    if isinstance(t, BooleanType):   return pa.bool_()
    if isinstance(t, IntegerType):   return pa.int32()
    if isinstance(t, LongType):      return pa.int64()
    if isinstance(t, FloatType):     return pa.float32()
    if isinstance(t, DoubleType):    return pa.float64()
    if isinstance(t, DateType):      return pa.date32()
    if isinstance(t, TimestampType): return pa.timestamp("us")
    if isinstance(t, StringType):    return pa.string()
    if isinstance(t, BinaryType):    return pa.binary()
    if isinstance(t, DecimalType):   return pa.decimal128(t.precision, t.scale)
    if isinstance(t, MapType):       return pa.map_(iceberg_to_arrow_type(t.key_type), iceberg_to_arrow_type(t.value_type))
    if isinstance(t, ListType):      return pa.list_(iceberg_to_arrow_type(t.element_type))
    if isinstance(t, StructType):    return pa.struct([pa.field(f.name, iceberg_to_arrow_type(f.field_type)) for f in t.fields])
    raise TypeError(f"Unsupported Iceberg type: {t}")

def table_arrow_schema(tbl: Table) -> pa.Schema:
    return pa.schema([
        pa.field(f.name, iceberg_to_arrow_type(f.field_type), nullable=f.optional)
        for f in tbl.schema().fields
    ])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def lower_dotted(name: str) -> str:
    if name in ("source_nickname", "event_date"):
        return name
    if "." in name:
        a, b = name.split(".", 1)
        return f"{a.lower()}.{b.lower()}"
    return name.lower()

def camel_to_flat_lower(s: str) -> str:
    # "timeCCingOthers" -> "timeccingothers"; "summoner1Id" -> "summoner1id"
    return re.sub(r'[^A-Za-z0-9]', '', s).lower()

def participant_api_key_to_col(k: str) -> str:
    return f"participant.{camel_to_flat_lower(k)}"

def cast_map_number_keep_keys(d: Dict[str, Any], *, context: str = "") -> Dict[str, float]:
    """ĞĞ¸ĞºĞ°ĞºĞ¸Ğµ ĞºĞ»ÑÑ‡Ğ¸ Ğ½Ğµ Ğ´Ñ€Ğ¾Ğ¿Ğ°ĞµĞ¼ â€” Ğ²ÑĞµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº Ñ‡Ğ¸ÑĞ»Ğ°Ğ¼ Ğ¿Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ñƒ."""
    if not isinstance(d, dict):
        raise TypeError(f"{context}: expected dict, got {type(d).__name__}")
    out: Dict[str, float] = {}
    for k, v in d.items():
        try:
            if isinstance(v, bool):
                out[str(k)] = float(int(v))
            elif isinstance(v, (int, float)):
                out[str(k)] = float(v)
            elif isinstance(v, str):
                try:
                    out[str(k)] = float(v)
                except Exception:
                    out[str(k)] = float(len(v))
            elif isinstance(v, (list, tuple, set)):
                out[str(k)] = float(len(v))
            elif isinstance(v, dict):
                out[str(k)] = float(len(v))
            else:
                out[str(k)] = float(len(str(v)))
        except Exception as e:
            logging.warning("%s: failed to cast key '%s' of type %s (%s) â†’ use 0.0",
                            context, k, type(v).__name__, e)
            out[str(k)] = 0.0
    return out

def normalize_perks(perks: Dict[str, Any]) -> Dict[str, Any]:
    stat = perks["statPerks"]
    styles = perks["styles"]
    norm_styles = []
    for s in styles:
        sel = s["selections"]
        norm_sel = []
        for it in sel:
            norm_sel.append({
                "perk": int(it["perk"]),
                "var1": int(it["var1"]),
                "var2": int(it["var2"]),
                "var3": int(it["var3"]),
            })
        norm_styles.append({
            "style": int(s["style"]),
            "description": s.get("description"),
            "selections": norm_sel,
        })
    return {
        "statperks": {"offense": int(stat["offense"]), "flex": int(stat["flex"]), "defense": int(stat["defense"])},
        "styles": norm_styles,
    }

def split_required_optional(tbl: Table) -> Tuple[List[str], List[str]]:
    required, optional = [], []
    for f in tbl.schema().fields:
        (optional if f.optional else required).append(f.name)
    return required, optional

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTTP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def safe_get(url: str, headers: Dict[str, str], *, max_retries: int = 3, backoff: float = 0.7) -> Optional[Dict[str, Any]]:
    for attempt in range(max_retries):
        try:
            r = requests.get(url, headers=headers, timeout=15, verify=False)
        except requests.RequestException as exc:
            logging.error("ğŸ’¥ %s â€” network error: %s", url, exc)
            return None
        if r.status_code == 200:
            try:
                return r.json()
            except ValueError:
                logging.error("ğŸ’¥ %s â€” invalid JSON: %.120s", url, r.text)
                return None
        if r.status_code == 429 and attempt < max_retries - 1:
            retry_after = int(r.headers.get("Retry-After", 1))
            time.sleep(retry_after + backoff)
            continue
        logging.error("ğŸ’¥ %s â€” HTTP %s: %.120s", url, r.status_code, r.text)
        return None
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Riot â†’ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_rows_for_day(riot_id: str, day: dt.date) -> List[Dict[str, Any]]:
    headers = {"X-Riot-Token": RIOT_API_KEY}
    riot_id_clean = re.sub(r"[\u2066-\u2069]", "", riot_id)

    game_name, tagline = riot_id_clean.split("#", 1)

    acct_url = (
        f"https://{REGIONAL_ROUTING}.api.riotgames.com/riot/account/v1/accounts/by-riot-id/"
        f"{urllib.parse.quote(game_name)}/{urllib.parse.quote(tagline)}"
    )
    acct = safe_get(acct_url, headers) or {}
    puuid = acct["puuid"]  # fail-fast

    start_ts = int(dt.datetime.combine(day, dt.time()).timestamp())
    end_ts   = start_ts + 86400
    ids_url = (
        f"https://{REGIONAL_ROUTING}.api.riotgames.com/lol/match/v5/matches/by-puuid/{puuid}/ids"
        f"?startTime={start_ts}&endTime={end_ts}&count=100"
    )
    match_ids: List[str] = safe_get(ids_url, headers) or []

    logging.info("ğŸ¯ [%s] [%s] Riot: %d matches", riot_id_clean, day.isoformat(), len(match_ids))

    rows: List[Dict[str, Any]] = []
    for mid in match_ids:
        m = safe_get(f"https://{REGIONAL_ROUTING}.api.riotgames.com/lol/match/v5/matches/{mid}", headers) or {}

        base_src = pd.json_normalize(m)
        base = {lower_dotted(c): base_src.at[0, c] for c in META_COLS}
        if len(base) != len(META_COLS):
            missing = [lower_dotted(c) for c in META_COLS if c not in base_src.columns]
            raise RuntimeError(f"Missing META columns from API: {missing}")

        for p in m["info"]["participants"]:
            row: Dict[str, Any] = {}
            row.update(base)

            for k, v in p.items():
                col = participant_api_key_to_col(k)
                if col == "participant.challenges":
                    row[col] = cast_map_number_keep_keys(v, context=f"match={mid} participant.challenges")
                elif col == "participant.missions":
                    tmp = cast_map_number_keep_keys(v, context=f"match={mid} participant.missions")
                    row[col] = {kk: int(vv) for kk, vv in tmp.items()}
                elif col == "participant.perks":
                    row[col] = normalize_perks(v)
                else:
                    row[col] = v

            row["source_nickname"] = riot_id_clean
            row["event_date"] = day
            rows.append(row)

    logging.info("ğŸ§± [%s] [%s] Built rows: %d", riot_id_clean, day.isoformat(), len(rows))
    return rows

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ¹ â†’ Ğ°Ğ½Ñ‚Ğ¸Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_existing_keys(tbl: Table, nick: str, day: dt.date) -> Set[Tuple[str, str]]:
    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ (event_date, source_nickname)
    expr = And(EqualTo("event_date", day), EqualTo("source_nickname", nick))

    # â¬‡ï¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ²Ğ¾ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ PyIceberg
    scan = tbl.scan(
        row_filter=expr,
        selected_fields=["metadata.matchid", "participant.puuid"],
    )

    existing: Set[Tuple[str, str]] = set()

    try:
        # Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ: Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Arrow-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†ĞµĞ¹ (ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ Ğ² Ñ‚Ğ²Ğ¾ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸)
        at = scan.to_arrow()
        if at is not None and at.num_rows > 0:
            df = at.to_pandas()
            for mid, puuid in zip(df["metadata.matchid"], df["participant.puuid"]):
                existing.add((mid, puuid))
    except Exception as e:
        logging.warning("âš ï¸ fetch_existing_keys: scan.to_arrow() Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» (%s). ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ½Ğ° post-plan Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ.", e)

    # fallback: Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¿Ğ¾Ñ€Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ñƒ (Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ²ĞµÑ€ÑĞ¸ÑÑ…)
    if not existing:
        try:
            for task in scan.plan_files():
                try:
                    at = task.to_arrow()
                    if at is not None and at.num_rows > 0:
                        df = at.to_pandas()
                        for mid, puuid in zip(df["metadata.matchid"], df["participant.puuid"]):
                            existing.add((mid, puuid))
                except Exception as e2:
                    logging.warning("âš ï¸ fetch_existing_keys: task.to_arrow() Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ: %s", e2)
        except Exception as e3:
            logging.warning("âš ï¸ fetch_existing_keys: plan_files() Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ: %s", e3)

    logging.info("ğŸ” [%s] [%s] Existing keys in table: %d", nick, day.isoformat(), len(existing))
    return existing
    # Ğ¡Ğ¾Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¿Ğ°Ñ€Ñ‹ (nick, day), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞºĞ°Ğ½Ğ¾Ğ²
    pairs: Set[Tuple[str, dt.date]] = set(
        (r["source_nickname"], r["event_date"]) for r in rows
    )

    # Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ ĞºĞ»ÑÑ‡Ğ¸ Ğ¸Ğ· Iceberg
    existing_map: Dict[Tuple[str, dt.date], Set[Tuple[str, str]]] = {}
    for nick, day in pairs:
        existing_map[(nick, day)] = fetch_existing_keys(tbl, nick, day)

    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸
    kept: List[Dict[str, Any]] = []
    dropped = 0
    for r in rows:
        nick = r["source_nickname"]
        day  = r["event_date"]
        key  = (r["metadata.matchid"], r["participant.puuid"])
        if key in existing_map[(nick, day)]:
            dropped += 1
        else:
            kept.append(r)

    if pairs:
        # Ğ”Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¾Ğ² ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾ Ğ¾Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼
        pairs_str = ", ".join([f"{n} {d.isoformat()}" for (n, d) in sorted(pairs)])
        logging.info("ğŸ§¹ Dedupe for {%s}: kept=%d, dropped=%d", pairs_str, len(kept), dropped)
    else:
        logging.info("ğŸ§¹ Dedupe: no pairs found")

    return kept

def fetch_existing_keys(tbl: Table, nick: str, day: dt.date) -> Set[Tuple[str, str]]:
    """Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ»ÑÑ‡Ğ¸ (matchId, puuid) Ğ´Ğ»Ñ (nick, day) Ğ¸Ğ· Iceberg, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ÑƒĞ±Ğ»Ğ¸."""
    expr = And(EqualTo("event_date", day), EqualTo("source_nickname", nick))
    scan = tbl.scan(
        row_filter=expr,
        selected_fields=["metadata.matchid", "participant.puuid"],
    )

    existing: Set[Tuple[str, str]] = set()

    # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ: Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Arrow-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†ĞµĞ¹
    try:
        at = scan.to_arrow()
        if at is not None and at.num_rows > 0:
            df = at.to_pandas()
            for mid, puuid in zip(df["metadata.matchid"], df["participant.puuid"]):
                existing.add((mid, puuid))
    except Exception as e:
        logging.warning("âš ï¸ fetch_existing_keys: scan.to_arrow() Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» (%s).", e)

    # Fallback: Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¿Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ñƒ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸
    if not existing:
        try:
            for task in scan.plan_files():
                try:
                    at = task.to_arrow()
                    if at is not None and at.num_rows > 0:
                        df = at.to_pandas()
                        for mid, puuid in zip(df["metadata.matchid"], df["participant.puuid"]):
                            existing.add((mid, puuid))
                except Exception as e2:
                    logging.warning("âš ï¸ fetch_existing_keys: task.to_arrow() Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ: %s", e2)
        except Exception as e3:
            logging.warning("âš ï¸ fetch_existing_keys: plan_files() Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ: %s", e3)

    logging.info("ğŸ” [%s] [%s] Existing keys in table: %d", nick, day.isoformat(), len(existing))
    return existing


def dedupe_rows_by_keys(rows: List[Dict[str, Any]], tbl: Table) -> List[Dict[str, Any]]:
    """ĞĞ½Ñ‚Ğ¸Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚: Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ (nick, day) Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ ÑÑ‚Ñ€Ğ¾ĞºĞ¸,
    Ñ‡ÑŒĞ¸ (metadata.matchid, participant.puuid) ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ."""
    if not rows:
        return rows

    pairs: Set[Tuple[str, dt.date]] = set((r["source_nickname"], r["event_date"]) for r in rows)

    existing_map: Dict[Tuple[str, dt.date], Set[Tuple[str, str]]] = {}
    for nick, day in pairs:
        existing_map[(nick, day)] = fetch_existing_keys(tbl, nick, day)

    kept: List[Dict[str, Any]] = []
    dropped = 0
    for r in rows:
        nick = r["source_nickname"]
        day  = r["event_date"]
        key  = (r["metadata.matchid"], r["participant.puuid"])
        if key in existing_map[(nick, day)]:
            dropped += 1
        else:
            kept.append(r)

    if pairs:
        pairs_str = ", ".join([f"{n} {d.isoformat()}" for (n, d) in sorted(pairs)])
        logging.info("ğŸ§¹ Dedupe for {%s}: kept=%d, dropped=%d", pairs_str, len(kept), dropped)
    else:
        logging.info("ğŸ§¹ Dedupe: no pairs found")

    return kept
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def append_rows_strict(rows: List[Dict[str, Any]]) -> int:
    if not rows:
        return 0

    tbl = load_table()
    # ĞĞ½Ñ‚Ğ¸Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚ Ğ¿Ğ¾ (metadata.matchid, participant.puuid) Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ·Ğµ (event_date, source_nickname)
    rows = dedupe_rows_by_keys(rows, tbl)

    if not rows:
        logging.info("â„¹ï¸ All rows are duplicates â€” nothing to append.")
        return 0

    required_cols, optional_cols = split_required_optional(tbl)
    all_cols_in_order = [f.name for f in tbl.schema().fields]

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ REQUIRED
    for r in rows:
        missing_req = [c for c in required_cols if c not in r]
        if missing_req:
            raise RuntimeError(f"Missing REQUIRED columns in row: {missing_req[:8]}{'...' if len(missing_req)>8 else ''}")

    df = pd.DataFrame(rows)
    for c in optional_cols:
        if c not in df.columns:
            df[c] = None

    # ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ¾Ğ² Ñ€Ğ¾Ğ²Ğ½Ğ¾ ĞºĞ°Ğº Ğ² ÑÑ…ĞµĞ¼Ğµ
    df = df[all_cols_in_order]

    # event_date â†’ date
    if "event_date" in df.columns:
        df["event_date"] = pd.to_datetime(df["event_date"]).dt.date

    pa_tbl = pa.Table.from_pandas(df, schema=table_arrow_schema(tbl), preserve_index=False)
    tbl.append(pa_tbl)

    # Ğ›Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ğ½Ğ¸Ğº/Ğ´Ğ°Ñ‚Ğ°Ğ¼ Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğµ
    by_pair = df.groupby(["source_nickname", "event_date"]).size().reset_index(name="rows")
    for _, row in by_pair.iterrows():
        logging.info("âœ… APPEND: [%s] [%s] rows=%d", row["source_nickname"], row["event_date"], int(row["rows"]))

    logging.info("ğŸ§¾ Appended total %d rows to %s", pa_tbl.num_rows, TABLE_ID)
    return pa_tbl.num_rows

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    today = dt.date.today()
    start = today - dt.timedelta(weeks=1)
    end   = today - dt.timedelta(days=1)

    riot_ids = [
        "Monty Gard#RU1",
        "Breaksthesilence#RU1",
        "2pilka#RU1",
        "Gruntq#RU1",
        "Ğ¨aĞ·Ğ°Ğ¼#RU1",
        "Prooaknor#RU1",
    ]

    total = 0
    for riot in riot_ids:
        day = start
        while day <= end:
            try:
                logging.info("ğŸšš START: nick=[%s] day=[%s]", riot, day.isoformat())
                rows = build_rows_for_day(riot_id=riot, day=day)
                total += append_rows_strict(rows)
            except Exception:
                logging.exception("ğŸ’¥ Error on %s for %s", day, riot)
            day += dt.timedelta(days=1)

    logging.info("âœ… Done. Total rows written: %d", total)
